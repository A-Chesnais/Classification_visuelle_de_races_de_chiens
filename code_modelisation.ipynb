{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preprocessing-:\" data-toc-modified-id=\"Preprocessing-:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preprocessing :</a></span><ul class=\"toc-item\"><li><span><a href=\"#Modèle-utilisé-:\" data-toc-modified-id=\"Modèle-utilisé-:-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Modèle utilisé :</a></span></li><li><span><a href=\"#Sans-cropping\" data-toc-modified-id=\"Sans-cropping-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Sans cropping</a></span></li><li><span><a href=\"#Avec-cropping\" data-toc-modified-id=\"Avec-cropping-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Avec cropping</a></span></li><li><span><a href=\"#Data-augmentation\" data-toc-modified-id=\"Data-augmentation-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Data augmentation</a></span></li></ul></li><li><span><a href=\"#Tests-de-différents-modèles-:\" data-toc-modified-id=\"Tests-de-différents-modèles-:-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Tests de différents modèles :</a></span><ul class=\"toc-item\"><li><span><a href=\"#Définition-des-générateurs-de-données-:\" data-toc-modified-id=\"Définition-des-générateurs-de-données-:-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Définition des générateurs de données :</a></span></li><li><span><a href=\"#base-VGG-16-:\" data-toc-modified-id=\"base-VGG-16-:-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>base VGG 16 :</a></span><ul class=\"toc-item\"><li><span><a href=\"#Modèle-référence-:\" data-toc-modified-id=\"Modèle-référence-:-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Modèle référence :</a></span></li><li><span><a href=\"#Ajout-de-couches-de-convolutions-:\" data-toc-modified-id=\"Ajout-de-couches-de-convolutions-:-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Ajout de couches de convolutions :</a></span></li><li><span><a href=\"#Augmentation-du-nombre-de-filtres\" data-toc-modified-id=\"Augmentation-du-nombre-de-filtres-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Augmentation du nombre de filtres</a></span></li><li><span><a href=\"#Augmentation-de-la-taille-des-filtres\" data-toc-modified-id=\"Augmentation-de-la-taille-des-filtres-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>Augmentation de la taille des filtres</a></span></li><li><span><a href=\"#Optimisation\" data-toc-modified-id=\"Optimisation-2.2.5\"><span class=\"toc-item-num\">2.2.5&nbsp;&nbsp;</span>Optimisation</a></span></li></ul></li><li><span><a href=\"#Base-Residual-Network\" data-toc-modified-id=\"Base-Residual-Network-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Base Residual Network</a></span><ul class=\"toc-item\"><li><span><a href=\"#Définition-du-bloc-résiduel\" data-toc-modified-id=\"Définition-du-bloc-résiduel-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Définition du bloc résiduel</a></span></li><li><span><a href=\"#Définition-du-modèle\" data-toc-modified-id=\"Définition-du-modèle-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Définition du modèle</a></span></li><li><span><a href=\"#Régularisation-via-l2\" data-toc-modified-id=\"Régularisation-via-l2-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>Régularisation via l2</a></span></li><li><span><a href=\"#Régularisation-via-dropout\" data-toc-modified-id=\"Régularisation-via-dropout-2.3.4\"><span class=\"toc-item-num\">2.3.4&nbsp;&nbsp;</span>Régularisation via dropout</a></span></li></ul></li></ul></li><li><span><a href=\"#Transfert-learning\" data-toc-modified-id=\"Transfert-learning-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Transfert learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Utilitaires\" data-toc-modified-id=\"Utilitaires-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Utilitaires</a></span></li><li><span><a href=\"#Features-extractions-+-classifieur\" data-toc-modified-id=\"Features-extractions-+-classifieur-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Features extractions + classifieur</a></span><ul class=\"toc-item\"><li><span><a href=\"#InceptionV3\" data-toc-modified-id=\"InceptionV3-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>InceptionV3</a></span></li><li><span><a href=\"#VGG16\" data-toc-modified-id=\"VGG16-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>VGG16</a></span></li><li><span><a href=\"#ResNet50V2\" data-toc-modified-id=\"ResNet50V2-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>ResNet50V2</a></span></li></ul></li><li><span><a href=\"#Fine-tuning-partiel-:\" data-toc-modified-id=\"Fine-tuning-partiel-:-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Fine tuning partiel :</a></span><ul class=\"toc-item\"><li><span><a href=\"#Resnet50\" data-toc-modified-id=\"Resnet50-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Resnet50</a></span></li><li><span><a href=\"#VGG16\" data-toc-modified-id=\"VGG16-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>VGG16</a></span></li><li><span><a href=\"#InceptionV3\" data-toc-modified-id=\"InceptionV3-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>InceptionV3</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Input,Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Activation,BatchNormalization, Dropout, Flatten, Dense, add\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'C:/Users/Antoine/Documents/Git/Projet 6/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle utilisé :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = Sequential()\n",
    "\n",
    "test_model.add(Conv2D(16, (3, 3), input_shape=(224, 224, 3), kernel_initializer='he_uniform'))\n",
    "test_model.add(Activation('relu'))\n",
    "test_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "test_model.add(Conv2D(32, (3, 3), kernel_initializer='he_uniform'))\n",
    "test_model.add(Activation('relu'))\n",
    "test_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "test_model.add(Flatten())  \n",
    "test_model.add(Dense(250))\n",
    "test_model.add(Activation('relu'))\n",
    "test_model.add(Dropout(0.5))\n",
    "test_model.add(Dense(120))\n",
    "test_model.add(Activation('softmax'))\n",
    "\n",
    "test_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "test_model.save_weights(directory_path + 'data_keras/preprocess_weight.h5')\n",
    "\n",
    "#early_stopping_monitor = EarlyStopping(monitor='val_loss', patience=3, mode='min')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sans cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_base = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_it_separated = datagen_base.flow_from_directory(directory_path + 'data_keras/separated_data/train/', \n",
    "                                            class_mode='categorical', batch_size=64, target_size=(224, 224))\n",
    "val_it_separated  = datagen_base.flow_from_directory(directory_path + 'data_keras/separated_data/validation/', \n",
    "                                          class_mode='categorical', batch_size=64, target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.load_weights(directory_path + 'data_keras/preprocess_weight.h5')\n",
    "\n",
    "hist = test_model.fit_generator(train_it_separated, epochs=10, steps_per_epoch=220, validation_data=val_it_separated, \n",
    "                                    validation_steps=45, verbose=1)\n",
    "\n",
    "for key in hist.history:\n",
    "    hist.history[key] = [float(np_float) for np_float in hist.history[key]]\n",
    "json.dump(hist.history, open(directory_path +'models/test_data_base.txt', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avec cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_it_cropped = datagen_base.flow_from_directory(directory_path + 'data_keras/data_cropped/train/', \n",
    "                                            class_mode='categorical', batch_size=64, target_size=(224, 224))\n",
    "val_it_cropped  = datagen_base.flow_from_directory(directory_path + 'data_keras/data_cropped/validation/', \n",
    "                                          class_mode='categorical', batch_size=64, target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.load_weights(directory_path + 'data_keras/preprocess_weight.h5')\n",
    "\n",
    "hist = test_model.fit_generator(train_it_cropped, epochs=10, steps_per_epoch=220, validation_data=val_it_cropped, \n",
    "                                    validation_steps=45, verbose=1)\n",
    "\n",
    "for key in hist.history:\n",
    "    hist.history[key] = [float(np_float) for np_float in hist.history[key]]\n",
    "json.dump(hist.history, open(directory_path +'models/test_data_cropping.txt', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_aug =  ImageDataGenerator( rotation_range=20, width_shift_range=0.15, height_shift_range=0.15, \n",
    "                                  rescale=1./255, shear_range=0.15, zoom_range=0.2, horizontal_flip=True, \n",
    "                                  fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_it_augmented = datagen_base.flow_from_directory(directory_path + 'data_keras/data_augmented/train/', \n",
    "                                                class_mode='categorical', batch_size=64, target_size=(224, 224))\n",
    "val_it_augmented = datagen_base.flow_from_directory(directory_path + 'data_keras/data_augmented/validation/', \n",
    "                                              class_mode='categorical', batch_size=64, target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.load_weights(directory_path + 'data_keras/preprocess_weight.h5')\n",
    "\n",
    "hist = test_model.fit_generator(train_it_augmented, epochs=20, steps_per_epoch=220, validation_data=val_it_augmented, \n",
    "                                    validation_steps=45, verbose=1)\n",
    "\n",
    "for key in hist.history:\n",
    "    hist.history[key] = [float(np_float) for np_float in hist.history[key]]\n",
    "json.dump(hist.history, open(directory_path +'models/test_data_augment.txt', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests de différents modèles :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour cette partie, afin d'accélérer les temps de calcul, la data augmentation a été effectuée au préalable et les images ont été sauvegardées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition des générateurs de données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_it = datagen.flow_from_directory(directory_path + 'data_keras/data_augmented/train/', class_mode='categorical', batch_size=64, target_size=(224, 224))\n",
    "val_it = datagen.flow_from_directory(directory_path + 'data_keras/data_augmented/validation/', class_mode='categorical', batch_size=64, target_size=(224, 224))\n",
    "test_it = datagen.flow_from_directory(directory_path + 'data_keras/data_augmented/test/', class_mode='categorical', batch_size=64, target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## base VGG 16 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle référence :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = Sequential()\n",
    "\n",
    "base_model.add(Conv2D(16, (3, 3), input_shape=(224, 224, 3), kernel_initializer='he_uniform'))\n",
    "base_model.add(Activation('relu'))\n",
    "base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "base_model.add(Conv2D(32, (3, 3), kernel_initializer='he_uniform'))\n",
    "base_model.add(Activation('relu'))\n",
    "base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "base_model.add(Flatten())  \n",
    "base_model.add(Dense(250))\n",
    "base_model.add(Activation('relu'))\n",
    "base_model.add(Dropout(0.5))\n",
    "base_model.add(Dense(120))\n",
    "base_model.add(Activation('softmax'))\n",
    "\n",
    "base_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "base_model_hist = base_model.fit_generator(train_it, epochs=30, steps_per_epoch=200, validation_data=val_it, validation_steps=40, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.save(directory_path + 'models/base_model.h5')\n",
    "for key in base_model_hist.history:\n",
    "    base_model_hist.history[key] = [float(np_float) for np_float in base_model_hist.history[key]]\n",
    "json.dump(base_model_hist.history, open(directory_path +'models/base_model_hist.txt', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajout de couches de convolutions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_conv_model = Sequential()\n",
    "\n",
    "aug_conv_model.add(Conv2D(16, (3, 3), input_shape=(224, 224, 3), kernel_initializer='he_uniform'))\n",
    "aug_conv_model.add(Activation('relu'))\n",
    "aug_conv_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "aug_conv_model.add(Conv2D(32, (3, 3), kernel_initializer='he_uniform'))\n",
    "aug_conv_model.add(Activation('relu'))\n",
    "aug_conv_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "aug_conv_model.add(Conv2D(64, (3, 3), kernel_initializer='he_uniform'))\n",
    "aug_conv_model.add(Activation('relu'))\n",
    "aug_conv_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "aug_conv_model.add(Conv2D(128, (3, 3), kernel_initializer='he_uniform'))\n",
    "aug_conv_model.add(Activation('relu'))\n",
    "aug_conv_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "aug_conv_model.add(Flatten())  \n",
    "aug_conv_model.add(Dense(250))\n",
    "aug_conv_model.add(Activation('relu'))\n",
    "aug_conv_model.add(Dropout(0.5))\n",
    "aug_conv_model.add(Dense(120))\n",
    "aug_conv_model.add(Activation('softmax'))\n",
    "\n",
    "aug_conv_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aug_conv_model_hist = aug_conv_model.fit_generator(train_it, epochs=50, steps_per_epoch=200, validation_data=val_it, validation_steps=40, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_conv_model.save(directory_path + 'models/aug_conv_model.h5')\n",
    "for key in aug_conv_model_hist.history:\n",
    "    aug_conv_model_hist.history[key] = [float(np_float) for np_float in aug_conv_model_hist.history[key]]\n",
    "json.dump(aug_conv_model_hist.history, open(directory_path +'models/aug_conv_model_hist.txt', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation du nombre de filtres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_nb_filter_model = Sequential()\n",
    "\n",
    "aug_nb_filter_model.add(Conv2D(32, (3, 3), input_shape=(224, 224, 3), kernel_initializer='he_uniform'))\n",
    "aug_nb_filter_model.add(Activation('relu'))\n",
    "aug_nb_filter_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "aug_nb_filter_model.add(Conv2D(64, (3, 3), kernel_initializer='he_uniform'))\n",
    "aug_nb_filter_model.add(Activation('relu'))\n",
    "aug_nb_filter_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "aug_nb_filter_model.add(Conv2D(128, (3, 3), kernel_initializer='he_uniform'))\n",
    "aug_nb_filter_model.add(Activation('relu'))\n",
    "aug_nb_filter_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "aug_nb_filter_model.add(Conv2D(256, (3, 3), kernel_initializer='he_uniform'))\n",
    "aug_nb_filter_model.add(Activation('relu'))\n",
    "aug_nb_filter_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "aug_nb_filter_model.add(Flatten())  \n",
    "aug_nb_filter_model.add(Dense(250))\n",
    "aug_nb_filter_model.add(Activation('relu'))\n",
    "aug_nb_filter_model.add(Dropout(0.5))\n",
    "aug_nb_filter_model.add(Dense(120))\n",
    "aug_nb_filter_model.add(Activation('softmax'))\n",
    "\n",
    "aug_nb_filter_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aug_nb_filter_model_hist = aug_nb_filter_model.fit_generator(train_it, epochs=50, steps_per_epoch=200, validation_data=val_it, validation_steps=40, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_nb_filter_model.save(directory_path + 'models/aug_nb_filter_model.h5')\n",
    "for key in aug_nb_filter_model_hist.history:\n",
    "    aug_nb_filter_model_hist.history[key] = [float(np_float) for np_float in aug_nb_filter_model_hist.history[key]]\n",
    "json.dump(aug_nb_filter_model_hist.history, open(directory_path +'models/aug_nb_filter_model_hist.txt', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation de la taille des filtres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_filter_size_model = Sequential()\n",
    "\n",
    "aug_filter_size_model.add(Conv2D(16, (7, 7), input_shape=(224, 224, 3), kernel_initializer='he_uniform'))\n",
    "aug_filter_size_model.add(Activation('relu'))\n",
    "aug_filter_size_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "aug_filter_size_model.add(Conv2D(32, (5, 5), kernel_initializer='he_uniform'))\n",
    "aug_filter_size_model.add(Activation('relu'))\n",
    "aug_filter_size_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "aug_filter_size_model.add(Conv2D(64, (5, 5), kernel_initializer='he_uniform'))\n",
    "aug_filter_size_model.add(Activation('relu'))\n",
    "aug_filter_size_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "aug_filter_size_model.add(Conv2D(128, (3, 3), kernel_initializer='he_uniform'))\n",
    "aug_filter_size_model.add(Activation('relu'))\n",
    "aug_filter_size_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "aug_filter_size_model.add(Flatten())  \n",
    "aug_filter_size_model.add(Dense(250))\n",
    "aug_filter_size_model.add(Activation('relu'))\n",
    "aug_filter_size_model.add(Dropout(0.5))\n",
    "aug_filter_size_model.add(Dense(120))\n",
    "aug_filter_size_model.add(Activation('softmax'))\n",
    "\n",
    "aug_filter_size_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aug_filter_size_model_hist = aug_filter_size_model.fit_generator(train_it, epochs=50, steps_per_epoch=200, validation_data=val_it, validation_steps=40, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_filter_size_model.save(directory_path + 'models/aug_filter_size_model.h5')\n",
    "for key in aug_filter_size_model_hist.history:\n",
    "    aug_filter_size_model_hist.history[key] = [float(np_float) for np_float in aug_filter_size_model_hist.history[key]]\n",
    "json.dump(aug_filter_size_model_hist.history, open(directory_path +'models/aug_filter_size_model_hist.txt', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Définition du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_custom_vgg(p_drop):\n",
    "    \n",
    "    final_model = Sequential()\n",
    "\n",
    "    final_model.add(Conv2D(32, (3, 3), kernel_initializer='he_uniform'))\n",
    "    final_model.add(Activation('relu'))\n",
    "    final_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    final_model.add(Conv2D(32, (3, 3), kernel_initializer='he_uniform'))\n",
    "    final_model.add(Activation('relu'))\n",
    "    final_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    final_model.add(Conv2D(64, (3, 3), kernel_initializer='he_uniform'))\n",
    "    final_model.add(Activation('relu'))\n",
    "    final_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    final_model.add(Conv2D(64, (3, 3), kernel_initializer='he_uniform'))\n",
    "    final_model.add(Activation('relu'))\n",
    "    final_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    final_model.add(Conv2D(128, (3, 3), kernel_initializer='he_uniform'))\n",
    "    final_model.add(Activation('relu'))\n",
    "    final_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    final_model.add(Conv2D(256, (3, 3), kernel_initializer='he_uniform'))\n",
    "    final_model.add(Activation('relu'))\n",
    "    final_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    final_model.add(Flatten())\n",
    "    final_model.add(Dense(500))\n",
    "    final_model.add(Activation('relu'))\n",
    "    final_model.add(Dropout(p_drop))\n",
    "    final_model.add(Dense(250))\n",
    "    final_model.add(Activation('relu'))\n",
    "    final_model.add(Dropout(p_drop))\n",
    "    final_model.add(Dense(120))\n",
    "    final_model.add(Activation('softmax'))\n",
    "    \n",
    "    return final_model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Niveau de régularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dropout_list = [0.1,0.3,0.5]\n",
    "for p_drop in dropout_list:\n",
    "    \n",
    "    final_model = def_custom_VGG(p_drop)\n",
    "\n",
    "    final_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    final_model_hist = final_model.fit_generator(train_it, epochs=100, steps_per_epoch=200, \n",
    "                                                 validation_data=val_it, validation_steps=40, verbose=1)\n",
    "    \n",
    "    final_model.save(directory_path + 'models/final_model_' + str(p_drop*10)[0] + '.h5')\n",
    "    for key in final_model_hist.history:\n",
    "        final_model_hist.history[key] = [float(np_float) for np_float in final_model_hist.history[key]]\n",
    "    json.dump(final_model_hist.history, open(directory_path +'models/final_model_' + str(p_drop*10)[0] + '.txt', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_opt = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "rms_opt = optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
    "sgd_high = optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False)\n",
    "sgd_low = optimizers.SGD(learning_rate=0.001, momentum=0.0, nesterov=False)\n",
    "sgd_momentum = optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=False)\n",
    "\n",
    "optimizer_list = [adam_opt, rms_opt, sgd_high, sgd_low, sgd_momentum]\n",
    "optimizer_name = ['adam', 'rmsprop', 'sgd_high', 'sgd_low', 'sgd_momentum']\n",
    "\n",
    "for opti,name in zip(optimizer_list,optimizer_name):\n",
    "    \n",
    "    final_model = def_custom_vgg(0.3)\n",
    "\n",
    "    final_model.compile(loss='categorical_crossentropy', optimizer=opti, metrics=['accuracy'])\n",
    "    \n",
    "    final_model_hist = final_model.fit_generator(train_it, epochs=100, steps_per_epoch=200, \n",
    "                                                 validation_data=val_it, validation_steps=40, verbose=1)\n",
    "    \n",
    "    final_model.save(directory_path + 'models/final_model_' + name + '.h5')\n",
    "    for key in final_model_hist.history:\n",
    "        final_model_hist.history[key] = [float(np_float) for np_float in final_model_hist.history[key]]\n",
    "    json.dump(final_model_hist.history, open(directory_path +'models/final_model_' + name + '.txt', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Residual Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition du bloc résiduel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_module(layer_in, n_filters, s, reg_alpha):\n",
    "    merge_input = layer_in\n",
    "\n",
    "    if layer_in.shape[-1] != n_filters:\n",
    "        merge_input = Conv2D(n_filters, (1,1), strides=(s,s), padding='same', kernel_regularizer=reg_alpha, kernel_initializer='he_uniform')(layer_in)\n",
    "    \n",
    "    conv1 = Conv2D(n_filters, (3,3), padding='same', strides=(1,1), kernel_regularizer=reg_alpha, kernel_initializer='he_uniform')(layer_in)\n",
    "    \n",
    "    activ_1 = Activation('relu')(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(n_filters, (3,3), padding='same', strides=(s,s), kernel_regularizer=reg_alpha, kernel_initializer='he_uniform')(activ_1)\n",
    "    \n",
    "    layer_out = add([conv2, merge_input])\n",
    "    \n",
    "    layer_out = Activation('relu')(layer_out)\n",
    "    return layer_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_res_model(reg_alpha):\n",
    "    \n",
    "    image_flat = Input(shape=(224, 224, 3))\n",
    "    layer_1 = Conv2D(kernel_size=7, filters=16, strides=2, padding='same', kernel_initializer='he_uniform')(image_flat)\n",
    "    layer_2 = Activation('relu')(layer_1)\n",
    "\n",
    "    block_4 = residual_module(layer_2, 32, s=2, reg_alpha=reg_alpha)\n",
    "    block_5 = residual_module(block_4, 32, s=1, reg_alpha=reg_alpha)\n",
    "    block_6 = residual_module(block_5, 32, s=1, reg_alpha=reg_alpha)\n",
    "    block_7 = residual_module(block_6, 64, s=2, reg_alpha=reg_alpha)\n",
    "    block_8 = residual_module(block_7, 64, s=1, reg_alpha=reg_alpha)\n",
    "    block_9 = residual_module(block_8, 64, s=1, reg_alpha=reg_alpha)\n",
    "    block_10 = residual_module(block_9, 128, s=2, reg_alpha=reg_alpha)\n",
    "    block_11 = residual_module(block_10, 256, s=2, reg_alpha=reg_alpha)\n",
    "    layer_12 = AveragePooling2D((2,2), name='avg_pool') (block_11)\n",
    "    \n",
    "    layer_13 = Flatten()(layer_12)\n",
    "    layer_14 = Dense(250, activation='relu')(layer_13)\n",
    "    out = Dense(120, activation='softmax')(layer_14)\n",
    "    model_res =  Model(inputs=image_flat, outputs=out)\n",
    "    \n",
    "    return model_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_res = def_res_model(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_res.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_res_hist = model_res.fit_generator(train_it, epochs=50, steps_per_epoch=200, validation_data=val_it, validation_steps=40, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_res.save(directory_path + 'models/model_res.h5')\n",
    "for key in model_res_hist.history:\n",
    "    model_res_hist.history[key] = [float(np_float) for np_float in model_res_hist.history[key]]\n",
    "json.dump(model_res_hist.history, open(directory_path +'models/model_res_hist.txt', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Régularisation via l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha_list = [0.001,0.005,0.01,0.05]\n",
    "for alpha in alpha_list:\n",
    "\n",
    "    final_model = def_res_model(l2(alpha))\n",
    "\n",
    "    final_model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
    "    \n",
    "    final_model_hist = final_model.fit_generator(train_it, epochs=100, steps_per_epoch=200, \n",
    "                                                 validation_data=val_it, validation_steps=40, verbose=1)\n",
    "    \n",
    "    final_model.save(directory_path + 'models/final_res_model_L2_' + str(int(alpha*1000)) + '.h5')\n",
    "    for key in final_model_hist.history:\n",
    "        final_model_hist.history[key] = [float(np_float) for np_float in final_model_hist.history[key]]\n",
    "    json.dump(final_model_hist.history, open(directory_path +'models/final_res_model_L2_' + str(int(alpha*1000)) + '.txt', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Régularisation via dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_res_model_dropout(p_drop):\n",
    "    \n",
    "    image_flat = Input(shape=(224, 224, 3))\n",
    "    layer_1 = Conv2D(kernel_size=7, filters=16, strides=2, padding='same', kernel_initializer='he_uniform')(image_flat)\n",
    "    layer_2 = Activation('relu')(layer_1)\n",
    "\n",
    "    block_4 = residual_module(layer_2, 32, s=2, reg_alpha=None)\n",
    "    block_5 = residual_module(block_4, 32, s=1, reg_alpha=None)\n",
    "    block_6 = residual_module(block_5, 32, s=1, reg_alpha=None)\n",
    "    block_7 = residual_module(block_6, 64, s=2, reg_alpha=None)\n",
    "    block_8 = residual_module(block_7, 64, s=1, reg_alpha=None)\n",
    "    block_9 = residual_module(block_8, 64, s=1, reg_alpha=None)\n",
    "    block_10 = residual_module(block_9, 128, s=2, reg_alpha=None)\n",
    "    block_11 = residual_module(block_10, 256, s=2, reg_alpha=None)\n",
    "    layer_12 = AveragePooling2D((2,2), name='avg_pool') (block_11)\n",
    "    \n",
    "    layer_13 = Flatten()(layer_12)\n",
    "    layer_14 = Dense(250, activation='relu')(layer_13)\n",
    "    layer_15 = Dropout(p_drop)(layer_14)\n",
    "    out = Dense(120, activation='softmax')(layer_15)\n",
    "    model_res =  Model(inputs=image_flat, outputs=out)\n",
    "    \n",
    "    return model_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_drop_list = [0.1,0.3,0.5]\n",
    "for p_drop in p_drop_list:\n",
    "\n",
    "    final_model = def_res_model_dropout(p_drop)\n",
    "\n",
    "    final_model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "                                                                                    metrics=['accuracy'])\n",
    "    \n",
    "    final_model_hist = final_model.fit_generator(train_it, epochs=100, steps_per_epoch=200, \n",
    "                                                 validation_data=val_it, validation_steps=40, verbose=1)\n",
    "    \n",
    "    final_model.save(directory_path + 'models/final_res_model_dropout_' + str(p_drop*10)[0] + '.h5')\n",
    "    for key in final_model_hist.history:\n",
    "        final_model_hist.history[key] = [float(np_float) for np_float in final_model_hist.history[key]]\n",
    "    json.dump(final_model_hist.history, open(directory_path +'models/final_res_model_dropout_' + str(p_drop*10)[0] + '.txt', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_05_model = load_model(directory_path + '/backup/custom_resnet/final_res_model_dropout_5.h5')\n",
    "drop_05_model = drop_05_model.fit_generator(train_it, epochs=100, steps_per_epoch=200, \n",
    "                                            validation_data=val_it, validation_steps=40, \n",
    "                                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop_05_model.save(directory_path + 'models/final_res_model_dropout_extended.h5')\n",
    "for key in drop_05_model.history:\n",
    "    drop_05_model.history[key] = [float(np_float) for np_float in drop_05_model.history[key]]\n",
    "json.dump(drop_05_model.history, open(directory_path +'models/final_res_model_dropout_extended.txt', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfert learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "nb_train_samples = 14397\n",
    "nb_validation_samples = 3084"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_index(model,name):\n",
    "    for idx, layer in enumerate(model.layers):\n",
    "        if layer.name == name:\n",
    "            return(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_iterator(preprocessing, batch_size):\n",
    "    \n",
    "    datagen = ImageDataGenerator(preprocessing_function=preprocessing)\n",
    "    datagen_aug =  ImageDataGenerator( rotation_range=20, width_shift_range=0.15, height_shift_range=0.15\n",
    "                                      , shear_range=0.15, zoom_range=0.2, horizontal_flip=True, \n",
    "                                      fill_mode='nearest',preprocessing_function=preprocessing)\n",
    "    \n",
    "    train_it = datagen_aug.flow_from_directory(directory_path + 'data_keras/data_cropped/train/', class_mode='categorical', batch_size=batch_size, target_size=(224, 224))\n",
    "    val_it = datagen.flow_from_directory(directory_path + 'data_keras/data_cropped/validation/', class_mode='categorical', batch_size=batch_size, target_size=(224, 224))\n",
    "    test_it = datagen.flow_from_directory(directory_path + 'data_keras/data_cropped/test/', class_mode='categorical', batch_size=batch_size, target_size=(224, 224))\n",
    "    \n",
    "    return train_it, val_it, test_it\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bottleneck_features(model, repo, preprocessing, batch_size, nb_train_samples, nb_validation_samples):\n",
    "    \n",
    "    datagen = ImageDataGenerator(preprocessing_function=preprocessing)\n",
    "\n",
    "    generator = datagen.flow_from_directory(directory_path + 'data_keras/data_cropped/train/', target_size=(224, 224),\n",
    "                                            batch_size=batch_size, class_mode=None, shuffle=False)\n",
    "    \n",
    "    bottleneck_features_train = model.predict_generator( generator, nb_train_samples // batch_size)\n",
    "    \n",
    "    np.save(open(directory_path +'transfer_learning/'+ repo + '/bottleneck_features_train.npy', 'wb'), bottleneck_features_train)\n",
    "\n",
    "    generator = datagen.flow_from_directory(directory_path + 'data_keras/data_cropped/validation/', target_size=(224, 224), \n",
    "                                            batch_size=batch_size, class_mode=None, shuffle=False)\n",
    "    \n",
    "    bottleneck_features_validation = model.predict_generator(generator, nb_validation_samples // batch_size)\n",
    "    \n",
    "    np.save(open(directory_path +'transfer_learning/'+ repo + '/bottleneck_features_validation.npy', 'wb'), bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features extractions + classifieur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_bottleneck_features(applications.inception_v3.InceptionV3(weights='imagenet', include_top=False, pooling='avg'), \n",
    "                         'Inception', applications.inception_v3.preprocess_input, \n",
    "                         batch_size, nb_train_samples, nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_inception_top_model():\n",
    "    train_data = np.load(open(directory_path +'transfer_learning/Inception/bottleneck_features_train.npy', 'rb'))\n",
    "    train_labels = np.load(open(directory_path +'transfer_learning/Inception/bottleneck_labels_train.npy', 'rb'))[0:nb_train_samples // batch_size * batch_size]\n",
    "\n",
    "    validation_data = np.load(open(directory_path +'transfer_learning/Inception/bottleneck_features_validation.npy', 'rb'))\n",
    "    validation_labels = np.load(open(directory_path +'transfer_learning/Inception/bottleneck_labels_validation.npy', 'rb'))[0:nb_validation_samples // batch_size * batch_size]    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(240, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(120, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    hist = model.fit(train_data, train_labels, epochs=20, batch_size=batch_size, validation_data=(validation_data, validation_labels))\n",
    "    \n",
    "    model.save_weights(directory_path + 'transfer_learning/Inception/trl_top_inception_weights.h5')\n",
    "    for key in hist.history:\n",
    "        hist.history[key] = [float(np_float) for np_float in hist.history[key]]\n",
    "    json.dump(hist.history, open(directory_path +'transfer_learning/Inception/trl_top_inception.txt', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_inception_top_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_bottleneck_features(applications.VGG16( weights='imagenet', include_top=False, input_shape=(224,224,3)), \n",
    "                         'VGG', applications.vgg16.preprocess_input, \n",
    "                         batch_size, nb_train_samples, nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_VGG_top_model():\n",
    "    train_data = np.load(open(directory_path +'transfer_learning/VGG/bottleneck_features_train.npy', 'rb'))\n",
    "    train_labels = np.load(open(directory_path +'transfer_learning/VGG/bottleneck_labels_train.npy', 'rb'))[0:nb_train_samples // batch_size * batch_size]\n",
    "\n",
    "    validation_data = np.load(open(directory_path +'transfer_learning/VGG/bottleneck_features_validation.npy', 'rb'))\n",
    "    validation_labels = np.load(open(directory_path +'transfer_learning/VGG/bottleneck_labels_validation.npy', 'rb'))[0:nb_validation_samples // batch_size * batch_size]    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(120, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    hist =  model.fit(train_data, train_labels, epochs=20, batch_size=batch_size, validation_data=(validation_data, validation_labels))\n",
    "    \n",
    "    model.save_weights(directory_path + 'transfer_learning/VGG/trl_top_VGG_weights.h5')\n",
    "    for key in hist.history:\n",
    "        hist.history[key] = [float(np_float) for np_float in hist.history[key]]\n",
    "    json.dump(hist.history, open(directory_path + 'transfer_learning/VGG/trl_top_VGG.txt', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_VGG_top_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_bottleneck_features(applications.resnet_v2.ResNet50V2( weights='imagenet', include_top=False), \n",
    "                         'ResNet', applications.resnet_v2.preprocess_input, \n",
    "                         batch_size, nb_train_samples, nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet50_top_model():\n",
    "    \n",
    "    train_data = np.load(open(directory_path +'transfer_learning/ResNet/bottleneck_features_train.npy', 'rb'))\n",
    "    train_labels = np.load(open(directory_path +'transfer_learning/ResNet/bottleneck_labels_train.npy', 'rb'))[0:nb_train_samples // batch_size * batch_size]\n",
    "\n",
    "    validation_data = np.load(open(directory_path +'transfer_learning/ResNet/bottleneck_features_validation.npy', 'rb'))\n",
    "    validation_labels = np.load(open(directory_path +'transfer_learning/ResNet/bottleneck_labels_validation.npy', 'rb'))[0:nb_validation_samples // batch_size * batch_size]    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(240, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(120, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    hist = model.fit(train_data, train_labels, epochs=20, batch_size=batch_size, validation_data=(validation_data, validation_labels))\n",
    "    \n",
    "    model.save_weights(directory_path + 'transfer_learning/ResNet/trl_top_resnet50_weights.h5')\n",
    "    for key in hist.history:\n",
    "        hist.history[key] = [float(np_float) for np_float in hist.history[key]]\n",
    "    json.dump(hist.history, open(directory_path + 'transfer_learning/ResNet/trl_top_resnet50.txt', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_resnet50_top_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning partiel :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "nb_train_samples = 14397\n",
    "nb_validation_samples = 3084"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_it, val_it, test_it = def_iterator(applications.resnet_v2.preprocess_input, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_model = applications.resnet_v2.ResNet50V2( weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "print('Model loaded.')\n",
    "\n",
    "for layer in base_model.layers[:get_layer_index(base_model,'conv5_block1_preact_bn')]:\n",
    "    layer.trainable = False\n",
    "\n",
    "top_model = Sequential()\n",
    "top_model.add(GlobalAveragePooling2D(input_shape=(7,7,2048)))\n",
    "top_model.add(Dense(240, activation='relu'))\n",
    "top_model.add(Dropout(0.4))\n",
    "top_model.add(Dense(120, activation='softmax'))\n",
    "\n",
    "top_model.load_weights(directory_path + 'transfer_learning/ResNet/trl_top_resnet50_weights.h5')\n",
    "\n",
    "model = Model(inputs= base_model.input, outputs= top_model(base_model.output))\n",
    "\n",
    "model.compile(optimizer=optimizers.SGD(lr=1e-4, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit_generator(train_it, epochs=10, steps_per_epoch=nb_train_samples//batch_size, \n",
    "                    validation_data=val_it, validation_steps=nb_validation_samples//batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in hist.history:\n",
    "    hist.history[key] = [float(np_float) for np_float in hist.history[key]]\n",
    "json.dump(hist.history, open(directory_path + 'transfer_learning/ResNet/trl_partial_tuning_resnet_2.txt', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(directory_path + 'transfer_learning/ResNet/trl_partial_tuning_resnet.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "nb_train_samples = 14397\n",
    "nb_validation_samples = 3084"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_it, val_it, test_it = def_iterator(applications.vgg16.preprocess_input, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = applications.VGG16( weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "print('Model loaded.')\n",
    "\n",
    "for layer in base_model.layers[:get_layer_index(base_model,'block5_conv1')]:\n",
    "    layer.trainable = False\n",
    "\n",
    "top_model = Sequential()\n",
    "top_model.add(GlobalAveragePooling2D(input_shape=(7, 7, 512)))\n",
    "top_model.add(Dropout(0.4))\n",
    "top_model.add(Dense(120, activation='softmax'))\n",
    "\n",
    "top_model.load_weights(directory_path + 'transfer_learning/VGG/trl_top_VGG_weights.h5')\n",
    "\n",
    "model = Model(inputs= base_model.input, outputs= top_model(base_model.output))\n",
    "\n",
    "model.compile(optimizer=optimizers.SGD(lr=1e-4, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist = model.fit_generator(train_it, epochs=10, steps_per_epoch=nb_train_samples//batch_size, \n",
    "                    validation_data=val_it, validation_steps=nb_validation_samples//batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in hist.history:\n",
    "    hist.history[key] = [float(np_float) for np_float in hist.history[key]]\n",
    "json.dump(hist.history, open(directory_path + 'transfer_learning/VGG/trl_partial_tuning_vgg.txt', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(directory_path + 'transfer_learning/VGG/trl_partial_tuning_vgg.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "nb_train_samples = 14397\n",
    "nb_validation_samples = 3084"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14397 images belonging to 120 classes.\n",
      "Found 3084 images belonging to 120 classes.\n",
      "Found 3099 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "train_it, val_it, test_it = def_iterator(applications.inception_v3.preprocess_input, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = applications.inception_v3.InceptionV3(weights='imagenet', include_top=False, pooling='avg')\n",
    "print('Model loaded.')\n",
    "\n",
    "for layer in base_model.layers[:get_layer_index(base_model,'mixed9 (Concatenate) ')]:\n",
    "    layer.trainable = False\n",
    "\n",
    "top_model = Sequential()\n",
    "top_model.add(Dense(240, activation='relu', input_shape=(None,2048)))\n",
    "top_model.add(Dropout(0.4))\n",
    "top_model.add(Dense(120, activation='softmax'))\n",
    "\n",
    "top_model.load_weights(directory_path + 'transfer_learning/Inception/trl_top_inception_weights.h5')\n",
    "\n",
    "model = Model(inputs= base_model.input, outputs= top_model(base_model.output))\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit_generator(train_it, epochs=10, steps_per_epoch=nb_train_samples//batch_size, \n",
    "                    validation_data=val_it, validation_steps=nb_validation_samples//batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in hist.history:\n",
    "    hist.history[key] = [float(np_float) for np_float in hist.history[key]]\n",
    "json.dump(hist.history, open(directory_path + 'transfer_learning/Inception/trl_full_tuning_inception_2.txt', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "304.233px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "704.15px",
    "left": "1774px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
